<!DOCTYPE html>
<html lang="en">
    <head>
        
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />

        <meta name="description" content="Firstyear&#x27;s blog">
        

        <title>Firstyear&#x27;s blog-a-log</title>

        
          <link rel="alternate" type="application/rss+xml" title="RSS" href="https://fy.blackhats.net.au/rss.xml">
        

        
            <link rel="stylesheet" href="https://fy.blackhats.net.au/theme.css">
        
        
    </head>
    <body>
        <div class="content">
        
        
            <header>
                <div class="header-left">
                    <a href="https:&#x2F;&#x2F;fy.blackhats.net.au" class="logo">Firstyear&#x27;s blog-a-log</a>
                </div>
                <div class="header-right">
                    <nav itemscope itemtype="http://schema.org/SiteNavigationElement">
                      <ul>
                        
                        
                            
                            <li class="nav">
                                <a itemprop="url" href="https://fy.blackhats.net.au/blog/">
                                    <span itemprop="name">Blog</span>
                                </a>
                            </li>
                        
                            
                            <li class="nav">
                                <a itemprop="url" href="https://fy.blackhats.net.au/pages/">
                                    <span itemprop="name">Pages</span>
                                </a>
                            </li>
                        
                        
                        <li class="nav">
                            <a itemprop="url" href="https://github.com/firstyear">
                                <img class="icon" src="https:&#x2F;&#x2F;fy.blackhats.net.au/icons/github.svg" alt="Github">
                            </a>
                        </li>
                        
                        <li class="nav">
                            <a itemprop="url" href="/rss.xml">
                                <img class="icon" src="https:&#x2F;&#x2F;fy.blackhats.net.au/rss-logo.png" alt="rss">
                            </a>
                        </li>
                      </ul>
                    </nav>
                </div>
            </header>
        
        
        <main>
            
<article itemscope itemtype="http://schema.org/BlogPosting">
    <div itemprop="headline">
        <h1>APFS (why is df showing me funny numbers?!)</h1>
        <div class="border"></div>
        <time datetime="2020-03-28" class="date" itemprop="datePublished">
            28 Mar 2020
        </time>
    </div>
    <div itemprop="articleBody">
        <h1 id="apfs-why-is-df-showing-me-funny-numbers">APFS (why is df showing me funny numbers?!)</h1>
<p>Apple's APFS has been the default for MacOS since High Sierra, where
SSD (flash) automatically would convert from HFS+. This is a god send,
especially with HFS+'s history of destroying any folder that has a
large number of inodes within it.</p>
<p>However, APFS behaves differently to previous filesystem technology.
Let's see if we can explain why df reports multiple 932Gi disks like
this:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>&gt; df -h
</span><span>Filesystem                             Size   Used  Avail Capacity    iused      ifree %iused  Mounted on
</span><span>/dev/disk1s5                          932Gi   10Gi  380Gi     3%     484322 9767493838    0%   /
</span><span>/dev/disk1s1                          932Gi  530Gi  380Gi    59%    2072480 9765905680    0%   /System/Volumes/Data
</span></code></pre>
<p>And if we can explain why when you delete large files, you don't get
any space back from df either.</p>
<h2 id="how-it-looked-with-hfs">How it looked with HFS+</h2>
<p>With HFS+ it was pretty simple. You had a disk (a block device), which
had partitions (slices of the space in the block device) and those
partitions were formatted with a filesystem that knew how to store data
in them. An example:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>&gt; diskutil list
</span><span>...
</span><span>/dev/disk2 (external, physical):
</span><span>   #:                       TYPE NAME                    SIZE       IDENTIFIER
</span><span>   0:      GUID_partition_scheme                         *1.0 TB     disk2
</span><span>   1:                        EFI EFI                     209.7 MB   disk2s1
</span><span>   2:                  Apple_HFS tmachine                999.9 GB   disk2s2
</span></code></pre>
<p>We can see that disk2 is 1.0TB in size, and it contains two partitions,
the first is 209.7MB for EFI (disk2s1) and the second has data and
formatted as HFS+ (disk2s2).</p>
<p>Of course, this has some drawbacks - partitions don't like being moved,
and filesystem resizing is a costly process of time and IO cycles. It's
quite inflexible. If you wanted another partition here for read only
data, well, you'd have to change a lot. Properties can only be applied
to a filesystem as a whole, and they can't share space. If you had a
1TB drive partitioned to 500GB each, and were running low on space on
one of them, well ... good luck! You have to move data manually, or
change where applications store data.</p>
<h2 id="apfs">APFS</h2>
<p>APFS doesn't quite follow this model though. APFS is what's called a
volume based filesystem. That means there is an intermediate layer in
here. The layout looks like this in diskutil</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>&gt; diskutil list
</span><span>...
</span><span>/dev/disk0 (internal, physical):
</span><span>   #:                       TYPE NAME                    SIZE       IDENTIFIER
</span><span>   0:      GUID_partition_scheme                        *1.0 TB     disk0
</span><span>   1:                        EFI EFI                     314.6 MB   disk0s1
</span><span>   2:                 Apple_APFS Container disk1         1.0 TB     disk0s2
</span></code></pre>
<p>So our disk0 looks like before - an EFI partition, and a very large APFS
container. However the container itself is NOT the filesystem. The
contain is a pool of storage that APFS volumes are created into. We can
see the volumes too.</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>&gt; diskutil list
</span><span>...
</span><span>/dev/disk1 (synthesized):
</span><span>   #:                       TYPE NAME                    SIZE       IDENTIFIER
</span><span>   0:      APFS Container Scheme -                      +1.0 TB     disk1
</span><span>                                 Physical Store disk0s2
</span><span>   1:                APFS Volume Macintosh HD â€” Data     569.4 GB   disk1s1
</span><span>   2:                APFS Volume Preboot                 81.8 MB    disk1s2
</span><span>   3:                APFS Volume Recovery                526.6 MB   disk1s3
</span><span>   4:                APFS Volume VM                      10.8 GB    disk1s4
</span><span>   5:                APFS Volume Macintosh HD            11.0 GB    disk1s5
</span></code></pre>
<p>Notice how /dev/disk1 is &quot;synthesized&quot;? It's not real - it's there
to &quot;trick&quot; legacy tools into thinking that the container is a
&quot;block&quot; device and the volumes are &quot;partitions&quot;.</p>
<h2 id="benefits-of-volumes">Benefits of Volumes</h2>
<p>One of the immediate benefits is that unlike partitions, in a volume
filesystem, <em>all</em> the space of the underlying container (also known as:
pool, volume group) is available to <em>all</em> volumes at anytime. Because
the volumes are a flexible concept, they can have non-contiguous
geometry on the disk (unlike a partition). That's why in your df output
you can see:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>&gt; df -h
</span><span>Filesystem                             Size   Used  Avail Capacity    iused      ifree %iused  Mounted on
</span><span>/dev/disk1s5                          932Gi   10Gi  380Gi     3%     484322 9767493838    0%   /
</span><span>/dev/disk1s1                          932Gi  530Gi  380Gi    59%    2072480 9765905680    0%   /System/Volumes/Data
</span></code></pre>
<p>Both disk1s5 (Macintosh HD) and disk1s1 (Macintosh HD --- Data) are APFS
volumes. The container has 932Gi total space, and 380Gi available in the
container which either volume could allocate. But you can also see the
exact space reservation of each volume too: disk1s5 only has 10Gi in
use, and disk1s1 has 530Gi in use.</p>
<p>It would be very possible for disk1s1 to grow to fill all the space, and
then to contract, and then have disk1s5 grow to take all the space and
contract - this is because the space is flexibly allocated from the
container. Neat!</p>
<p>Each volume also can have different properties applied. For example,
/dev/disk1s5 (Macintosh HD) in MacOS catalina is read-only:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>/dev/disk1s5 on / (apfs, local, read-only, journaled)
</span><span>/dev/disk1s1 on /System/Volumes/Data (apfs, local, journaled, nobrowse)
</span></code></pre>
<p>This is to prevent system tampering, and strengthen integrity of the
system. There are a number of tricks to achieve this such as overlaying
multiple volumes together. /Applications for example is actually a
folder consitituted from the content of /System/Applications and
/System/Volumes/Data/Applications. Anytime you &quot;drag&quot; and application
to /Applications, you are actually putting it into
/System/Volumes/Data/Applications. A very similar property holds for
/Users (/System/Volumes/Data/Users), and even /Volumes.</p>
<h2 id="copy-on-write-snapshots">Copy-on-Write, Snapshots</h2>
<p>APFS is also a copy-on-write filesystem. This means whenever you write
data, it's actually written to newly allocated disk regions, and the
pointers are atomicly flipped to it. The full write occurs or it does
not. This is part of the reason why APFS is so much better than HFS+ -
in a crash your data is either in a previous state, or the new state -
never a half written or corrupted state.</p>
<p>This is the reason why APFS is only used on SSD (flash) devices - COW is
very random IO write intensive, and on a rotational disk this would
cause the head to &quot;seek&quot; randomly which would make both writes and
reads very slow. SSD of course isn't affected by this, so having a
highly fragmented file does not impose a penalty in the same way.</p>
<p>Copy-on-Write however opens up some interesting behaviours. If you COW a
file, but never remove the old version, you have a <em>snapshot</em>. This
means you can have point-in-time views to how a filesystem was. This is
actually used now by time machine during backups to ensure the content
of a backup is stable before being written to the external backup media.
It also allow time machine to perform &quot;backups&quot; while you are
out-and-about, by snapshotting as you work. Because snapshots are just
&quot;not removing old data&quot; they are low overhead to maintain and take
snapshots.</p>
<p>You can see snapshots on your system with:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>&gt; tmutil listlocalsnapshots /
</span><span>Snapshots for volume group containing disk /:
</span><span>com.apple.TimeMachine.2020-03-27-084939.local
</span><span>com.apple.TimeMachine.2020-03-27-100157.local
</span><span>com.apple.TimeMachine.2020-03-27-105937.local
</span><span>com.apple.TimeMachine.2020-03-27-121414.local
</span><span>...
</span></code></pre>
<p>You can even take your own snapshots if you want!</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>&gt; time tmutil localsnapshot
</span><span>Created local snapshot with date: 2020-03-28-091943
</span><span>tmutil localsnapshot  0.01s user 0.01s system 4% cpu 0.439 total
</span></code></pre>
<p>See how fast that is! Remember also because this is based on
copy-on-write, the snapshots only take as much data as the
<em>differences</em>, or what you are changing as you work.</p>
<h2 id="space-reclaim">Space Reclaim</h2>
<p>This leads to the final point of confusion - when people delete files to
clear space, but df reports no change. For example:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>&gt; df -h
</span><span>Filesystem                             Size   Used  Avail Capacity    iused      ifree %iused  Mounted on
</span><span>/dev/disk1s1                          932Gi  530Gi  380Gi    59%    2072480 9765905680    0%   /System/Volumes/Data
</span><span>&gt; ls -alh Downloads/Windows_Server_2016_Datacenter_EVAL_en-us_14393_refresh.ISO
</span><span>-rwx------@ 1 william  staff   6.5G 10 Oct  2018 Downloads/Windows_Server_2016_Datacenter_EVAL_en-us_14393_refresh.ISO
</span><span>&gt; rm Downloads/Windows_Server_2016_Datacenter_EVAL_en-us_14393_refresh.ISO
</span><span>&gt; df -h
</span><span>Filesystem                             Size   Used  Avail Capacity    iused      ifree %iused  Mounted on
</span><span>/dev/disk1s1                          932Gi  530Gi  380Gi    59%    2072479 9765905681    0%   /System/Volumes/Data
</span></code></pre>
<p>Now I promise, I really did delete the file - check the &quot;iused&quot; and
&quot;ifree&quot; columns. But also note that the &quot;Used&quot; space didn't change?
Surely we should expect to see this value drop to 523Gi since I removed
a 6.5G file.</p>
<p>Remember that APFS is a voluming filesystem, with copy-on-write. Due to
snapshots, the space used in a volume is the sum of active data <em>and</em>
snapshotted data. This means that when you are removing a file you are
removing it from the volume at this point in time, but it may still
exist in snapshots that exist in the volume! That's why there is a
reduction in the iused/ifree (an inode pointer was removed) but no
change in the space (the file still exists in a snapshot).</p>
<p>During normal operation, provided there is sufficent freespace, you
won't actually notice this behaviour. But when you say ... have not a
lot of space left (maybe 10G), and you delete some files to import
something (say a 40G import), you try the copy again ... and it fails!
Drat! But you wait a bit and suddenly it works? What in heck happened?</p>
<p>In the background, MacOS has registered &quot;okay, the user demands at
least 30G more space to complete this task. Let's clean snapshots until
we have that much space available&quot;. The snapshots are pruned so when
you come back later, suddenly you have the space.</p>
<p>Again, you can actually do this yourself. tmutil has a command
&quot;thinlocalsnapshots&quot; for this. An example usage would be:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>&gt; tmutil thinlocalsnapshots /System/Volumes/Data [bytes required]
</span><span>Thinned local snapshots:
</span></code></pre>
<p>In my case I have a lot of space available, so no snapshots are pruned.
But you may find that multiple snapshots are removed in this process!</p>
<h2 id="conclusion">Conclusion</h2>
<p>APFS is actually a really cool piece of filesystem technology, and I
think has made MacOS one of the most viable platforms for reliable daily
use. It embraces many great ideas, and despite it's youth, has done
really well. But those new ideas conflict with legacy, and have some
behaviours that are not always clearly exposed on shown to you, the
user. Understanding those behaviours means we can see <em>why</em> our
computers are behaving in certain - sometimes unexpected - ways.</p>

    </div>
</article>

        </main>
        
        <footer>
            
            <div class="border"></div>
            <div class="footer">
                <small class="footer-left">
                    Copyright &copy; William Brown
                </small>
                <small class="footer-right">
                    Powered by <a href="https://www.getzola.org">Zola</a> | Theme <a href="https://github.com/barlog-m/oceanic-zen">Oceanic Zen</a>
                </small>
            </div>
        
        </footer>
    
        </div>
    </body>
</html>
