<!DOCTYPE html>
<html lang="en">
    <head>
        
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />

        <meta name="description" content="Firstyear&#x27;s blog">
        

        <title>Firstyear&#x27;s blog-a-log</title>

        
          <link rel="alternate" type="application/rss+xml" title="RSS" href="https://fy.blackhats.net.au/rss.xml">
        

        
            <link rel="stylesheet" href="https://fy.blackhats.net.au/theme.css">
        
        
    </head>
    <body>
        <div class="content">
        
        
            <header>
                <div class="header-left">
                    <a href="https:&#x2F;&#x2F;fy.blackhats.net.au" class="logo">Firstyear&#x27;s blog-a-log</a>
                </div>
                <div class="header-right">
                    <nav itemscope itemtype="http://schema.org/SiteNavigationElement">
                      <ul>
                        
                        
                            
                            <li class="nav">
                                <a itemprop="url" href="https://fy.blackhats.net.au/blog/">
                                    <span itemprop="name">Blog</span>
                                </a>
                            </li>
                        
                            
                            <li class="nav">
                                <a itemprop="url" href="https://fy.blackhats.net.au/pages/">
                                    <span itemprop="name">Pages</span>
                                </a>
                            </li>
                        
                        
                        <li class="nav">
                            <a itemprop="url" href="https://github.com/firstyear">
                                <img class="icon" src="https:&#x2F;&#x2F;fy.blackhats.net.au/icons/github.svg" alt="Github">
                            </a>
                        </li>
                        
                        <li class="nav">
                            <a itemprop="url" href="/rss.xml">
                                <img class="icon" src="https:&#x2F;&#x2F;fy.blackhats.net.au/rss-logo.png" alt="rss">
                            </a>
                        </li>
                      </ul>
                    </nav>
                </div>
            </header>
        
        
        <main>
            
<article itemscope itemtype="http://schema.org/BlogPosting">
    <div itemprop="headline">
        <h1>Recovering LVM when a device is missing with a cache pool lv</h1>
        <div class="border"></div>
        <time datetime="2019-11-26" class="date" itemprop="datePublished">
            26 Nov 2019
        </time>
    </div>
    <div itemprop="articleBody">
        <h1 id="recovering-lvm-when-a-device-is-missing-with-a-cache-pool-lv">Recovering LVM when a device is missing with a cache pool lv</h1>
<p>I had a heartstopping moment today: my after running a command lvm
proudly annouced it had removed an 8TB volume containing all of my
virtual machine backing stores.</p>
<h2 id="everyone-a-short-view-back-to-the-past">Everyone, A short view back to the past ...</h2>
<p>I have a home server, with the configured storage array of:</p>
<ul>
<li>2x 8TB SMR (Shingled Magnetic Recording) archive disks (backup
target)</li>
<li>2x 8TB disks (vm backing store)</li>
<li>2x 1TB nvme SSD (os + cache)</li>
</ul>
<p>The vm backing store also had a lvm cache segment via the nvme ssds in a
raid 1 configuration. This means that the 2x8TB drives are in raid 1,
and a partition on each of the nvme devices are in raid 1, then they are
composed to allow the nvme to cache blocks from/to the 8TB array.</p>
<p>Two weeks ago I noticed one of the nvme drives was producing IO errors
indicating a fault of the device. Not wanting to risk corruption or
other issues from growing out of hand, I immediately shutdown the
machine and identified the nvme disk with the error.</p>
<p>At this stage I took the precaution of imaging (dd) both the good and
bad nvme devices to the archive array. Subsequently I completed a secure
erase of the faulty nvme drive before returning it to the vendor for
RMA.</p>
<p>I then left the server offline as I was away from my home for more than
a week and would not need, and was unable to monitor if the other drives
would produce further errors.</p>
<h2 id="returning-home">Returning home ...</h2>
<p>I decided to ignore William of the past (always a bad idea) and to
&quot;break&quot; the raid on the remaining nvme device so that my server could
operate allowing me some options for work related tasks.</p>
<p>This is an annoying process in lvm - you need to remove the missing
device from the volume group as well as indicating to the array that it
should no longer be in a raid state. This vgreduce is only for removing
missing PV's, it shouldn't be doing anything else.</p>
<p>I initiated the raid break process on the home, root and swap devices.
The steps are:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>vgreduce --removemissing &lt;vgname&gt;
</span><span>lvconvert -m0 &lt;vgname&gt;/&lt;lvname&gt;
</span></code></pre>
<p>This occured without fault due to being present on an isolated
&quot;system&quot; volume group, so the partial lvs were untouched and left on
the remaining pv in the vg.</p>
<p>When I then initiated this process on the &quot;data&quot; vg which contained
the libvirt backing store, vgreduce gave me the terrifying message:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>Removing logical volume &quot;libvirt_t2&quot;.
</span></code></pre>
<p>Oh no ~</p>
<h2 id="recovery-attempts">Recovery Attempts</h2>
<p>When a logical volume is removed, it can be recovered as lvm stores
backups of the LVM metadata state in /etc/lvm/archive.</p>
<p>My initial first reaction was that I was on a live disk, so I needed to
backup this content else it would be lost on reboot. I chose to put this
on the unaffected, and healthy SMR archive array.</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>mount /dev/mapper/archive-backup /archive
</span><span>cp -a /etc/lvm /archive/lvm-backup
</span></code></pre>
<p>At this point I knew that randomly attempting commands would cause
<em>further damage</em> and likely <em>prevent any ability to recover</em>.</p>
<p>The first step was to activate ssh so that I could work from my laptop -
rather than the tty with keyboard and monitor on my floor. It also means
you can copy paste, which reduces errors. Remember, I'm booted on a
live usb, which is why I reset the password.</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span># Only needed in a live usb.
</span><span>passwd
</span><span>systemctl start sshd
</span></code></pre>
<p>I then formulated a plan and wrote it out. This helps to ensure that
I've thought through the recovery process and the risks, it helps be to
be fully aware of the situation.</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>vim recovery-plan.txt
</span></code></pre>
<p>Into this I laid out the commands I would follow. Here is the plan:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>bytes 808934440960
</span><span>data_00001-2096569583
</span><span>
</span><span>dd if=/dev/zero of=/mnt/lv_temp bs=4096 count=197493760
</span><span>losetup /dev/loop10 /mnt/lv_temp
</span><span>pvcreate --restorefile /etc/lvm/archive/data_00001-2096569583.vg --uuid iC4G41-PSFt-6vqp-GC0y-oN6T-NHnk-ivssmg /dev/loop10
</span><span>vgcfgrestore data --test --file /etc/lvm/archive/data_00001-2096569583.vg
</span></code></pre>
<p>Now to explain this: The situation we are in is:</p>
<ul>
<li>We have a removed data/libvirt_t2 logical volume</li>
<li>The VG data is missing a single PV (nvme0). It still has three PVs
(nvme1, sda1, sdb1).</li>
<li>We can not restore metadata unless all devices are present as per
the vgcfgrestore man page.</li>
</ul>
<p>This means, we need to make a replacement device to replace into the
array, and then to restore the metadata with that.</p>
<p>The &quot;bytes&quot; section you see, is the <em>size</em> of the missing nvme0
partition that was a member of this array - we need to create a loopback
device of the same or greater size to allow us to restore the metadata.
(dd, losetup)</p>
<p>Once the loopback is created, we can then recreate the pv on the
loopback device with the same UUID as the missing device.</p>
<p>Once this is present, we can now restore the metadata as documented
which should contain the logical volume.</p>
<p>I ran these steps and it was all great until vgcfgrestore. I can not
remember the exact error but it was along the lines of:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>Unable to restore metadata as PV was missing for VG when last modification was performed.
</span></code></pre>
<p>Yep, the vgreduce command has changed the VG state, triggering a
metadata backup, but because a device was missing at the time, we can
not restore this metadata.</p>
<h2 id="options">Options ...</h2>
<p>At this point I had to consider alternate options. I conducted research
into this topic as well to see if others had encountered this case (no
one has ever not been able to restore their metadata apparently in this
case ...). The options that I arrived at:</p>
<ul>
<li>
<ol>
<li>Restore the metadata from the nvme /root as it has older (but
known) states - however I had recently expanded the libvirt_t2
volume from a live disk, meaning it may not have the correct
part sizes.</li>
</ol>
</li>
<li>
<ol start="2">
<li>Attempt to extract the xfs filesystem with DD from the disk to
begin a data recovery.</li>
</ol>
</li>
<li>
<ol start="3">
<li>Cry in a corner</li>
</ol>
</li>
<li>
<ol start="4">
<li>Use lvcreate with the &quot;same paramaters&quot; and hope that it
aligns the start at the same location as the former
data/libvirt_t2 allowing the xfs filesystem to be accessed.</li>
</ol>
</li>
</ul>
<p>All of these weren't good - especially not 3.</p>
<p>I opted to attempt solution 1, and then if that failed, I would
disconnect one of the 8TB disks, attempt solution 4, then if that ALSO
failed, I would then attempt 2, finally all else lost I would begin
solution 3. The major risk of relying on 4 and 2 is that LVM has dynamic
geometry on disk, it does not always allocate contiguously. This means
that attempting 4 with lvcreate may not create with the same geometry,
and it may write to incorrect locations causing dataloss. The risk of 2
was again, due to the dynamic geometry what we recover may be
re-arranged and corrupt.</p>
<p>This mean option 1 was the best way to proceed.</p>
<p>I mounted the /root volume of the host and using the lvm archive I was
able to now restore the metadata.</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>vgcfgrestore data --test --file /system/etc/lvm/archive/data_00004-xxxx.vg
</span></code></pre>
<p>Once completed I performed an lvscan to refresh what block devices were
available. I was then shown that every member of the VG data had
conflicting seqno, and that the metadata was corrupt and unable to
proceed.</p>
<p>Somehow we'd made it worse :(</p>
<h2 id="successful-procedure">Successful Procedure</h2>
<p>At this point, faced with 3 options that were all terrible, I started to
do more research. I finally discovered a post describing that the lvm
metadata is stored on disk in the same format as the .vg files in the
archive, and it's a ring buffer. We may be able to restore from these.</p>
<p>To do so, you must <em>dd</em> out of the disk into a file, and then manipulate
the file to only contain a single metadata entry.</p>
<p>Remember how I made images of my disks before I sent them back? This was
their time to shine.</p>
<p>I did do a recovery plan with these commands too, but it was more
evolving due to the paramaters involved so it changed frequently with
the offsets. The plan was very similar to above - use a loop device as a
stand in for the missing block device, restore the metadata, and then go
from there.</p>
<p>We know that LVM metadata occurs in the first section of the disk, just
after the partition start. So to work out where this is we use gdisk to
show the partitions in the backup image.</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span># gdisk /mnt/mion.nvme0n1.img
</span><span>GPT fdisk (gdisk) version 1.0.4
</span><span>...
</span><span>
</span><span>Command (? for help): p
</span><span>Disk /mnt/mion.nvme0n1.img: 2000409264 sectors, 953.9 GiB
</span><span>Sector size (logical): 512 bytes
</span><span>...
</span><span>
</span><span>Number  Start (sector)    End (sector)  Size       Code  Name
</span><span>   1            2048         1026047   500.0 MiB   EF00
</span><span>   2         1026048       420456447   200.0 GiB   8E00
</span><span>   3       420456448      2000409230   753.4 GiB   8E00
</span></code></pre>
<p>It's important to note the sector size flag, as well as the fact the
output is in sectors.</p>
<p>The LVM header occupies 255 sectors after the start of the partition. So
this in mind we can now create a dd command to extract the needed
information.</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>dd if=/mnt/mion.nvme0n1.img of=/tmp/lvmmeta bs=512 count=255 skip=420456448
</span></code></pre>
<p>bs sets the sector size to 512, count will read from the start up to 255
sectors of size 'bs', and skip says to start reading after 'skip' *
'sector'.</p>
<p>At this point, we can now copy this and edit the file:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>cp /tmp/lvmmeta /archive/lvm.meta.edit
</span></code></pre>
<p>Within this file you can see the ring buffer of lvm metadata. You need
to find the highest seqno that is a <em>complete record</em>. For example, my
seqno = 20 was partial (is the lvm meta longer than 255, please contact
me if you know!), but seqno=19 was complete.</p>
<p>Here is the region:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span># ^ more data above.
</span><span>}
</span><span># Generated by LVM2 version 2.02.180(2) (2018-07-19): Mon Nov 11 18:05:45 2019
</span><span>
</span><span>contents = &quot;Text Format Volume Group&quot;
</span><span>version = 1
</span><span>
</span><span>description = &quot;&quot;
</span><span>
</span><span>creation_host = &quot;linux-p21s&quot;    # Linux linux-p21s 4.12.14-lp151.28.25-default #1 SMP Wed Oct 30 08:39:59 UTC 2019 (54d7657) x86_64
</span><span>creation_time = 1573459545      # Mon Nov 11 18:05:45 2019
</span><span>
</span><span>^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@data {
</span><span>id = &quot;4t86tq-3DEW-VATS-1Q5x-nLLy-41pR-zEWwnr&quot;
</span><span>seqno = 19
</span><span>format = &quot;lvm2&quot;
</span></code></pre>
<p>So from there you remove everything above &quot;contents = ...&quot;, and clean
up the vgname header. It should look something like this.</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>contents = &quot;Text Format Volume Group&quot;
</span><span>version = 1
</span><span>
</span><span>description = &quot;&quot;
</span><span>
</span><span>creation_host = &quot;linux-p21s&quot;    # Linux linux-p21s 4.12.14-lp151.28.25-default #1 SMP Wed Oct 30 08:39:59 UTC 2019 (54d7657) x86_64
</span><span>creation_time = 1573459545      # Mon Nov 11 18:05:45 2019
</span><span>
</span><span>data {
</span><span>id = &quot;4t86tq-3DEW-VATS-1Q5x-nLLy-41pR-zEWwnr&quot;
</span><span>seqno = 19
</span><span>format = &quot;lvm2&quot;
</span></code></pre>
<p>Similar, you need to then find the bottom of the segment (look for the
next highest seqno) and remove everything <em>below</em> the line: &quot;#
Generated by LVM2 ...&quot;</p>
<p>Now, you can import this metadata to the loop device for the missing
device. Note I had to wipe the former lvm meta segment due to the
previous corruption, which caused pvcreate to refuse to touch the
device.</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>dd if=/dev/zero of=/dev/loop10 bs=512 count=255
</span><span>pvcreate --restorefile lvmmeta.orig.nvme1.edited --uuid iC4G41-PSFt-6vqp-GC0y-oN6T-NHnk-ivssmg /dev/loop10
</span></code></pre>
<p>Now you can do a dry run:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>vgcfgrestore --test -f lvmmeta.orig.nvme1.edited data
</span></code></pre>
<p>And the real thing:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>vgcfgrestore -f lvmmeta.orig.nvme1.edited data
</span><span>lvscan
</span></code></pre>
<p>Hooray! We have volumes! Let's check them, and ensure their filesystems
are sane:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>lvs
</span><span>lvchange -ay data/libvirt_t2
</span><span>xfs_repair -n /dev/mapper/data-libvirt_t2
</span></code></pre>
<p>If xfs_repair says no errors, then go ahead and mount!</p>
<p>At this point, lvm started to resync the raid, so I'll leave that to
complete before I take any further action to detach the loopback device.</p>
<h2 id="how-to-handle-this-next-time">How to Handle This Next Time</h2>
<p>The cause of this issue really comes from vgreduce --removemissing
removing the device when a cache member can't be found. I plan to
report this as a bug.</p>
<p>However another key challenge was the inability to restore the lvm
metadata when the metadata archive reported a missing device. This is
what stopped me from being able to restore the array in the first place,
even though I had a &quot;fake&quot; replacement. This is also an issue I intend
to raise.</p>
<p>Next time I would:</p>
<ul>
<li>Activate the array as a partial</li>
<li>Remove the cache device first</li>
<li>Then stop the raid</li>
<li>Then perform the vgreduction</li>
</ul>
<p>I really hope this doesn't happen to you!</p>

    </div>
</article>

        </main>
        
        <footer>
            
            <div class="border"></div>
            <div class="footer">
                <small class="footer-left">
                    Copyright &copy; William Brown
                </small>
                <small class="footer-right">
                    Powered by <a href="https://www.getzola.org">Zola</a> | Theme <a href="https://github.com/barlog-m/oceanic-zen">Oceanic Zen</a>
                </small>
            </div>
        
        </footer>
    
        </div>
    </body>
</html>
